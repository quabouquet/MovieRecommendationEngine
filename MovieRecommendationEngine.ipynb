{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math,re\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, mean, udf, lit, current_timestamp, unix_timestamp, array_contains\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call spark server\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')  \n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ratings: 100836\n",
      "sample of ratings: \n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data:\n",
    "PATH_TO_DATA = 'D:\\\\PSU\\\\8thLaiOfferProjects\\\\03032019DSHomework02\\\\ml-latest-small'\n",
    "ratings = spark.read.csv(PATH_TO_DATA + '\\\\ratings.csv', header=True, inferSchema=True)\n",
    "ratings.cache()\n",
    "movies = spark.read.csv(PATH_TO_DATA + '\\\\movies.csv', header=True, inferSchema=True)\n",
    "movies.cache()\n",
    "links = spark.read.csv(PATH_TO_DATA + '\\\\links.csv', header=True, inferSchema=True)\n",
    "links.cache()\n",
    "\n",
    "print('number of ratings: %i' % ratings.count())\n",
    "print('sample of ratings: ')\n",
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of users:\n",
    "ratings.select('userID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9742"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of movies:\n",
    "movies.select('movieID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9724"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of movies rated by users:\n",
    "ratings.select('movieID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|movieID|\n",
      "+-------+\n",
      "|   3456|\n",
      "|   2939|\n",
      "|   7020|\n",
      "|   6668|\n",
      "|   7792|\n",
      "|  26085|\n",
      "|  34482|\n",
      "|  32160|\n",
      "|  30892|\n",
      "|   6849|\n",
      "|   1076|\n",
      "|  85565|\n",
      "|   3338|\n",
      "|  25855|\n",
      "|   4194|\n",
      "|   8765|\n",
      "|   5721|\n",
      "|  32371|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.createOrReplaceTempView('ratings_view')\n",
    "movies.createOrReplaceTempView('movies_view')\n",
    "\n",
    "# list movies not rated before\n",
    "spark.sql(   # run Spark.SQL query\n",
    "    '''SELECT DISTINCT movieID \n",
    "       FROM movies_view \n",
    "       WHERE movieID NOT IN (SELECT DISTINCT movieID FROM ratings_view)\n",
    "    ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 1032, localhost, executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat java.base/java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.base/java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat java.base/java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.base/java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-02a3c90a5a3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# genres_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmovies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'genres'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\LANGUAGE\\Anaconda3.5\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1360\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LANGUAGE\\Anaconda3.5\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LANGUAGE\\Anaconda3.5\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LANGUAGE\\Anaconda3.5\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LANGUAGE\\Anaconda3.5\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 1032, localhost, executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat java.base/java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.base/java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat java.base/java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:155)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:97)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.base/java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "# list movie genres:\n",
    "# genres_set = set(movies.select('genres').rdd.flatMap(tuple).flatMap(tuple).collect())\n",
    "# genres_set\n",
    "\n",
    "movies.select('genres').rdd.flatMap(tuple).flatMap(tuple).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Recommender System\n",
    "\n",
    "We use alternating least square (ALS) to train collaborative filtering model. To find the best parameters, grid search and 3-fold cross validation will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e92e2a46c343>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegressionEvaluator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mals_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mALS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# use grid search to tune parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LANGUAGE\\Anaconda3.5\\lib\\site-packages\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LANGUAGE\\Anaconda3.5\\lib\\site-packages\\pyspark\\ml\\recommendation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, rank, maxIter, regParam, numUserBlocks, numItemBlocks, implicitPrefs, alpha, userCol, itemCol, seed, ratingCol, nonnegative, checkpointInterval, intermediateStorageLevel, finalStorageLevel, coldStartStrategy)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m    166\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mALS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"org.apache.spark.ml.recommendation.ALS\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         self._setDefault(rank=10, maxIter=10, regParam=0.1, numUserBlocks=10, numItemBlocks=10,\n\u001b[0;32m    169\u001b[0m                          \u001b[0mimplicitPrefs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"user\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"item\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LANGUAGE\\Anaconda3.5\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[1;34m(java_class, *args)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \"\"\"\n\u001b[0;32m     62\u001b[0m         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mjava_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mjava_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\LANGUAGE\\Anaconda3.5\\lib\\site-packages\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36m_jvm\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot load _jvm from SparkContext. Is SparkContext initialized?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "als_obj = ALS()\n",
    "\n",
    "# use grid search to tune parameters\n",
    "param_grid = ParamGridBuilder()\\\n",
    "    .baseOn({als_obj.coldStartStrategy: 'drop'}) \\\n",
    "    .baseOn({als_obj.userCol: 'userId'}) \\\n",
    "    .baseOn({als_obj.itemCol: 'movieId'}) \\\n",
    "    .baseOn({als_obj.ratingCol: 'rating'}) \\\n",
    "    .baseOn({als_obj.maxIter: 20}) \\\n",
    "    .addGrid(als_obj.regParam, [0.001,0.05,0.1,0.5,1,10,100])\\\n",
    "    .addGrid(als_obj.rank, [6,8,10,12,14])\\\n",
    "    .build()\n",
    "\n",
    "# define evaluator with mse error\n",
    "mse_evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "# 3- fold cross validation\n",
    "cv_setting = CrossValidator(estoimator=als_obj, estimatorParamMaps=params_grid, evaluator=mse_evaluator, numFolds=3)\n",
    "cv_model = cv_setting.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of λ=0.1 and rank = 8 gives the lowest CV error = 0.762, and test error = 0.773.\n",
    "The result may slightly change due to the random seed.\n",
    "\n",
    "### Step 1: Data Wrangling:\n",
    "#### load rating and movie data:\n",
    "#### ratings: ratings data consists of about 100,000 ratings given by users to movies. Each row of the DataFrame consists of a userId, movieid, timestamp, together with the rating given by each user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark  # check pyspark is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "PATH_TO_DATA = 'D:\\\\PSU\\\\8thLaiOfferProjects\\\\03032019DSHomework02\\\\ml-latest-small'\n",
    "ratings = spark.read.csv(PATH_TO_DATA + '\\\\ratings.csv', header=True, inferSchema=True)\n",
    "ratings.cache()\n",
    "\n",
    "print('number of ratings: %i' % ratings.count())\n",
    "print('sample of ratings: ')\n",
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = spark.read.csv('/FileStore/tables/ratings.csv', header=True, inferSchema=True)\n",
    "ratings.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.select(\n",
    "  ratings.userId.cast('int'), ratings.movieId.cast('int'), ratings.rating.cast('float'), (ratings.timestamp.cast('long') * 1000).alias('timestamp'))\n",
    "ratings.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.groupBy('rating').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(x='rating', y='count', color='#3498db', data=ratings.groupBy('rating').count().toPandas())\n",
    "plt.savefig('fig_001.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average rating of movies\n",
    "ratings.groupBy('MovieId').agg(mean('rating')).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average rating of movies:\n",
    "ratings.groupBy('MovieId').agg(mean('rating')).toPandas()['avg(rating)'].plot(kind='hist', color='#3498db')\n",
    "plt.savefig('fig_002.png',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average rating from users\n",
    "ratings.groupBy('userId').agg(mean('rating')).toPandas()['avg(rating)'].plot(kind='hist', color='#3498db')\n",
    "plt.savefig('fig_003.png',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data from csv file\n",
    "raw_movies = spark.read.csv(PATH_TO_DATA + '\\\\movies.csv', header=True, inferSchema=True)\n",
    "\n",
    "print('number of rows %i' % raw_movies.count())\n",
    "print('raw movie data: ')\n",
    "raw_movies.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a UDF to convert the raw genres string to an array of genres and lowercase:\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "extract_genres = udf(lambda x: x.lower().split('|'), ArrayType(StringType()))\n",
    "\n",
    "# test:\n",
    "raw_movies.select('movieId', 'title', extract_genres('genres').alias('genres')).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a UDF to extract the release year from the title, and return the new tiele and year in a struct type:\n",
    "from pyspark.sql.types import StructType,StructField\n",
    "import re\n",
    "\n",
    "def extract_year_fn(title):\n",
    "    result = re.search('\\(\\d{4}\\)', title)\n",
    "    try:\n",
    "        if result:\n",
    "            group = result.group()\n",
    "            year = group[1:-1]\n",
    "            start_pos = result.start()\n",
    "            title = title[:start_pos-1]\n",
    "            return (title, year)\n",
    "        else:\n",
    "            return (title, 1970)\n",
    "    except:\n",
    "        print(title)\n",
    "    \n",
    "extract_year = udf(extract_year_fn,\\\n",
    "                  StructType([StructField('title', StringType(), True),\\\n",
    "                             StructField('release_date',StringType(), True)]))\n",
    "\n",
    "# extract the function\n",
    "a = 'Jumanji (1995)'\n",
    "extract_year_fn(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function works. Now, create a new DataFrame with the cleaned-up titles, release dates and genres of the movies\n",
    "movies = raw_movies.select(\n",
    "  'movieId', extract_year('title').title.alias('title'),\\\n",
    "  extract_year('title').release_date.alias('release_date'),\\\n",
    "  extract_genres('genres').alias('genres'))\n",
    "\n",
    "print('Cleaned movie data: ')\n",
    "movies.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data = spark.read.csv(PATH_TO_DATA + '\\\\links.csv', header=True, inferSchema=True)\n",
    "\n",
    "# join movies with links to get TMDB id\n",
    "movie_data = movies.join(link_data, movies.movieId == link_data.movieId)\\\n",
    "  .select(movies.movieId, movies.title, movies.release_date, movies.genres, link_data.tmdbId)\n",
    "\n",
    "num_movies = movie_data.count()\n",
    "\n",
    "print('Cleaned movie data with tmdbId links: ')\n",
    "movie_data.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display movie posters using TMDb API:\n",
    "\n",
    "try:\n",
    "#   from PIL import Image   unnecessary\n",
    "    import tmdbsimple as tmdb\n",
    "    from skimage import io\n",
    "\n",
    "    # replace this variable with your actual TMdb API key\n",
    "    tmdb.API_KEY = 'b3e7d17ea7101f51be7f876489c5398a'\n",
    "    print('successfully imported tmdbsimple')\n",
    "\n",
    "    # base URL for TMDB poster images\n",
    "    IMAGE_URL = 'http://image.tmdb.org/t/p/w185/'\n",
    "    movie_id = movie_data.first().tmdbId\n",
    "    movie_info = tmdb.Movies(movie_id).info()\n",
    "    movie_poster_url = IMAGE_URL + movie_info['poster_path']\n",
    "    image = io.imread(movie_poster_url)\n",
    "    io.imshow(image)\n",
    "    display(io.show())\n",
    "#   display(Image(movie_poster_url, width=200))   unnecessary\n",
    "except Exception:\n",
    "    print('Cannot import tmdbsimple, no movie posters will be displayed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Write data in Elasticsearch\n",
    "\n",
    "run ./bin/elasticsearch from command line to start elasticsearch on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'DESKTOP-03OPD3G',\n",
       " 'cluster_name': 'elasticsearch',\n",
       " 'cluster_uuid': '2W72Db4xT2a83hRig8uBgQ',\n",
       " 'version': {'number': '7.0.0',\n",
       "  'build_flavor': 'unknown',\n",
       "  'build_type': 'unknown',\n",
       "  'build_hash': 'b7e28a7',\n",
       "  'build_date': '2019-04-05T22:55:32.697037Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.0.0',\n",
       "  'minimum_wire_compatibility_version': '6.7.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# test ES instance is running\n",
    "es = Elasticsearch()\n",
    "es.info(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es.indices.delete(index='movie_rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index={\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                # This configures the custom analyzer we need to parse vectors \n",
    "                # such that the scoring plugin will work correctly.\n",
    "                'payload_analyzer': {\n",
    "                    'type': 'custom',\n",
    "                    'tokenizer': 'whitespace',\n",
    "                    'filter': 'delimited_payload_filter'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "        'ratings':{\n",
    "            # this mapping definition sets up the fields for the rating events\n",
    "            'properties':{\n",
    "                'timestamp':{\n",
    "                    'type':'date'\n",
    "                },\n",
    "                'userId': {\n",
    "                    'type': 'integer'\n",
    "                },\n",
    "                'movieId':{\n",
    "                    'type': 'integer'\n",
    "                },\n",
    "                'rating': {\n",
    "                    'type': 'float'\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'users': {\n",
    "            # this mapping definition sets up metadata fields for the users\n",
    "            'properties':{\n",
    "                'userId':{\n",
    "                    'type': 'integer'\n",
    "                },\n",
    "                '@model': {\n",
    "                    # this mapping definition sets up the fields for user factor vectors of our model\n",
    "                    'properties':{\n",
    "                        'factor':{\n",
    "                            'type': 'text',\n",
    "                            'term_vector': 'with_position_offsets_payloads',\n",
    "                            'analyzer': 'payload_analyzer'\n",
    "                        },\n",
    "                        'version': {\n",
    "                            'type': 'keyword'\n",
    "                        },\n",
    "                        'timestamp': {\n",
    "                            'type': 'date'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'movies': {\n",
    "            # this mapping definition sets up the metadata fields for the movies\n",
    "            'properties': {\n",
    "                'movieId': {\n",
    "                    'type': 'integer'\n",
    "                },\n",
    "                'genres': {\n",
    "                    'type': 'keyword'\n",
    "                },\n",
    "                'release_date': {\n",
    "                    'type': 'date',\n",
    "                    'format': 'year'\n",
    "                },\n",
    "                '@model': {\n",
    "                    # this definition mapping sets up fields for movie factor verctors of our model\n",
    "                    'properties': {\n",
    "                        'factor': {\n",
    "                            'type': 'text',\n",
    "                            'term_vecor': 'with_positions_offsets_payloads',\n",
    "                            'analyzer': 'payload_analyzer'\n",
    "                        },\n",
    "                        'version': {\n",
    "                            'type': 'keyword'\n",
    "                        },\n",
    "                        'timestamp':{\n",
    "                            'type': 'date'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Ratings and movies dataframes into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write rating data\n",
    "ratings.write.format('es').save('movie_rec/ratings')\n",
    "\n",
    "# write movie data, specifying the dataframe column to use the id mapping\n",
    "# each document has an unique id\n",
    "# if it is not specified, an id will be assigned by Elasticsearch\n",
    "movie_data.write.format('es').option('es.mapping.id', 'movieId').save('movie_rec/movies')\n",
    "\n",
    "# check load went okay\n",
    "print('Movie DF count: %d' % movie_data.count())\n",
    "print('ES index count: %d' % es.count(index='movie_rec, doc_type='movie)['count'])\n",
    "\n",
    "# check write went okay\n",
    "print('Rating DF count: %d' % ratings.count())\n",
    "print('ES index count: %d' % es.count(index='movie_rec', doc_type='ratings')['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test things out by retrieving 3 rating documents from Elasticsearch\n",
    "es.search(index='movie_rec', doc_type='ratings', q='*', size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use es to query documents\n",
    "# e.g. count ratings with timestamp between 2016-01-01 and 2016-02-01\n",
    "es.count(index='movie_rec', doc_type='ratings', q='timestamp:[2016-01-01 TO 2016-02-01]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test things out by searching for movies containing 'matrix' in the title\n",
    "es.search(index='movie_rec', doc_type='movies', q='titlematrix', size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Train a recommender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_from_es = spark.read.format('es').load('movie_rec/ratings')\n",
    "\n",
    "# train text split\n",
    "train, test = ratings_from_es.randomSplit([0.7, 0.3], seed=123)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_obj = ALS()\n",
    "\n",
    "# use grid search to tune parameters\n",
    "param_grid = ParamGridBuilder()\\\n",
    "    .basedOn({als_obj.coldStartStartegy: 'drop'})\\\n",
    "    .basedOn({als_obj.userCol: 'userId'})\\\n",
    "    .basedOn({als_obj.itemCol: 'movieId'})\\\n",
    "    .basedOn({als_obj.ratingCol: 'rating'})\\\n",
    "    .basedOn({als_obj.maxIter: 20})\\\n",
    "    .addGrid(als_obj.regParam, [0.001,0.05,0.1,0.5,1,10,100])\\\n",
    "    .addGrid(als_obj.rank, [6,8,10,12,14])\\\n",
    "    .build()\n",
    "\n",
    "# define evaluator with MSE error\n",
    "mse_evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "# 3- fold cross validation\n",
    "cv_setting = CrossValidator(estoimator=als_obj, estimatorParamMaps=params_grid, evaluator=mse_evaluator, numFolds=3)\n",
    "cv_model = cv_setting.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv_model.bestModel\n",
    "model.userFactors.show(5)\n",
    "model.itemFactors.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('parameters of the best model: \\n()'.format(cv_setting.getEstimatorParamMaps()[np.argmin(cv_model.avgMetrics)]))\n",
    "print('---------')\n",
    "print('cross validation RMSE of the best model: ()'.format(min(cv_model.avgMetrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(list(cv_model.avgMetrics[:]))\n",
    "res = []\n",
    "for x in [1e-3,0.05,0.1,0.5,1,10,100]:\n",
    "    for y in [6,8,10,12,14]:\n",
    "        res.append((x, y, next(a)))\n",
    "for item in res:\n",
    "    print('|' + '|'.join(list(map(str, item))) + '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_evaluator.evaluate((model.transform(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Export ALS user and item factor vectors to Elasticsearch\n",
    "\n",
    "Next step s to export the model factors (shown in the DataFrames above) to Elasticsearch.\n",
    "\n",
    "Inorder to store the model in the correct format for the index mappings set up, you will need to create some utility functions. These functions will allow you to convert the raw vectors (which are equivalent to a Python list in the factor DataFrame above) to the correct delimited string format. This ensures Elasticsearch will parse the vector field in the model correctly using the delimited token filter custom analyzer you configured earlier.\n",
    "\n",
    "You'll also create a function to convert a vector and related metadata (such as the Spark model id and a timestamp) into a DataFrame field that matches the model field in the Elasticsearch index mapping.\n",
    "##### Utility functions for converting factor vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vector(x):\n",
    "    '''convert a list or numpy array to delimited token filter format'''\n",
    "    return ''.join(['%s|%s' % (i, v) for i, v in enumerate(x)])\n",
    "\n",
    "def reverse_convert(s):\n",
    "    '''convert a delimited token filter format string back to list format'''\n",
    "    return [float(f.split('|')[1]) for t in s.split(' ')]\n",
    "\n",
    "def vector_to_struct(x, verstion, ts):\n",
    "    '''convert a vector to a SparkSQL Struct with string-format vector and version fields'''\n",
    "    return (convert_vector(x), version, ts)\n",
    "\n",
    "vector_struct = udf(vector_to_struct,\\\n",
    "                   StructType([StructField('factor', StringType(), True),\\\n",
    "                              StructField('version', StringTyep(), True),\\\n",
    "                              StructField('timestamp', LongType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_vector([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out the vector conversion function\n",
    "test_vec = model.userFactors.select('feature').first().features\n",
    "print(test_vec)\n",
    "print()\n",
    "print(convert_vector(test_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert factor vectors to [factor, version, timestamp] form and write to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = model.uid\n",
    "ts = unix_timestamp(current_timestamp())\n",
    "\n",
    "movie_vectors = model.itemFactors.select('id', vector_struct('feature', lit(ver), ts).alias('@model'))\n",
    "movie_vectors.select('id', '@model.factor', '@model.version', '@model.timestamp').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to ES, use:\n",
    "# - 'id' as the column to map to ES movie id\n",
    "# - 'update' write mode for ES, since you want to update new fields only\n",
    "# - 'append' write mode for Spark\n",
    "movie_vectors.write.format('es')\\\n",
    "    .option('es.mapping.id', 'id') \\\n",
    "    .option('es.write.operation', 'update') \\\n",
    "    .save('movie_rec/movies', mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to ES, use:\n",
    "# - 'id' as the column to map to ES movie id\n",
    "# - 'update' write mode for ES, since you want to update new fields only\n",
    "# - 'append' write mode for Spark\n",
    "user_vectors.write.format('es')\\\n",
    "    .option('es.mapping.id', 'id') \\\n",
    "    .option('es.write.operation', 'update') \\\n",
    "    .save('movie_rec/users', mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data was written correctly\n",
    "\n",
    "You can search for a movie to see if the model factor vector was written correctly. You should see a '@model': {'factor': '0|..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for a particular sci-fi movie\n",
    "es.search(index='movie_rec', doc_type='movies', q='star wars phanton menace', size=1)['hits']['hit'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Recommend using Elasticsearch\n",
    "\n",
    "Now that you have loaded your recommendation model into Elasticsearch, you will generate some recommendations. First, you'll need to create a few utility functions for:\n",
    "* Fetching movie posters from TMDb API (optional)\n",
    "* Constructing the Elasticsearch function score query to generate recommendations from your factor model\n",
    "* Given a movie, use this query to find the movies most similar to it\n",
    "* Given a user, use this query to find the movies with the highest predicted rating to recommend to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML, display\n",
    "\n",
    "def get_poster_url(id):\n",
    "    '''Fectch movie poster image URL from TMDb API given a tmdbId'''\n",
    "    IMAGE_URL = 'https://image.tmdb.org/t/p/w500'\n",
    "    try:\n",
    "        import tmdbsimple as tmdb\n",
    "        from tmdbsimple import APIKeyError\n",
    "        try: \n",
    "            movie = tmdb.Movies(id).info()\n",
    "            poster_url = IMAGE_URL + movie['poster_path'] if 'poster_path' in movie and movie['poster_path'] is not None\n",
    "            return poster_url\n",
    "        except APIKeyError as ae:\n",
    "            return 'KEY_ERROR'\n",
    "    except: Exception as me:\n",
    "            return 'NA'\n",
    "        \n",
    "def fn_query(query_vec, q='*', cosine=False):\n",
    "    '''\n",
    "    construct an Elasticsearch function score query.\n",
    "    \n",
    "    The query takes as parameters:\n",
    "        - the field in the candidate document that contains the factor vector\n",
    "        - the query vector\n",
    "        - a flag indicating whether to use dot product or consine similarity (normalized dot product) for scores\n",
    "    \n",
    "    The query vector passed in will be the user factor vector (if generating recommended movies for a user)\n",
    "    or movie factor vector (if generating similar movies for a given movie)\n",
    "    '''\n",
    "    return {\n",
    "        'query': {\n",
    "            'function_score': {\n",
    "                'query': {\n",
    "                    'query_string': {\n",
    "                        'query': q\n",
    "                    }\n",
    "                },\n",
    "                'script_score': {\n",
    "                    'script': {\n",
    "                        'inline': 'payload_vector_score',\n",
    "                        'lang': 'native',\n",
    "                        'params': {\n",
    "                            'field': '@model.factor',\n",
    "                            'vector': query_vec,\n",
    "                            'cosine': cosine\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                'boost_mode': 'replace'\n",
    "            }\n",
    "        }\n",
    "    }  # 1:45.73\n",
    "\n",
    "def get_similar(the_id, q='*', num=10, index='movie_rec', dt='movies'):\n",
    "    '''\n",
    "    Given a movie id, execute the recommendation function score query to find similar movies, ranked by cosine similarity\n",
    "    '''\n",
    "    response = es.get(index=index, doc_type=dt, id=the_id)\n",
    "    src = response['_source']\n",
    "    if '@model' in src and 'factor' in src['@model']:\n",
    "        raw_vec = src['@model']['factor']\n",
    "        # our script actually uses the list form for the query vector and handles conversion internally\n",
    "        query_vec = reverse_convert(raw_vec)\n",
    "        q = fn_query(query_vec, q=q, cosine=True)\n",
    "        results = es.search(index, dt, body=q)\n",
    "        hits = results['hits']['hits']\n",
    "        return src, hits[1:num+1]\n",
    "\n",
    "def get_user_recs(the_id, q='*', num=10, index='movie_rec'):\n",
    "    '''\n",
    "    Given a user id, execute the recommendation function score query to find top movies, ranked by predicted rating\n",
    "    '''\n",
    "    response = es.get(index=index, doc_type='users', id=the_id)\n",
    "    src = response['_source']\n",
    "    if '@model' in src and 'factor' in src['@model']:\n",
    "        raw_vec = src['@model']['factor']\n",
    "        # our script actually uses the list form for the query vector and handles conversion internally\n",
    "        query_vec = reverse_convert(raw_vec)\n",
    "        q = fn_query(query_vec, q=q, cosine=False)\n",
    "        results = es.search(index, 'movies', body=q)\n",
    "        hits = results['hits']['hits']\n",
    "        return src, hits[:num]\n",
    "        \n",
    "def get_movies_for_user(the_id, num=10, index='movie_rec'):\n",
    "    '''Given a user id, get the movie rated by that user, from highest to lowest rated\n",
    "    '''\n",
    "    response = es.search(index=index, doc_type='ratings',q='userId:%s' % the_id, size=num, sort=['rating:desc'])\n",
    "    hits = response['hits']['hits']\n",
    "    ids = [h['_source']['movieId'] for h in hits]\n",
    "    movies = es.mget(body=('ids':ids), index=index, doc_type='movies', _source_include=['tmdbId', 'title'])\n",
    "    movies_hits = movies['docs']\n",
    "    tmdbids = [h['_source'] for h in movies_hits]\n",
    "    return tmdbids\n",
    "\n",
    "def display_user_recs(the_id, q='*', num=10, num_last=10, index='movie_rec'):\n",
    "    user, recs = get_user_recs(the_id, q, num, index)\n",
    "    user_movies = get_movies_for_user(the_id, num_last, index)\n",
    "    # check that posters can be displayed\n",
    "    first_movie = user_movies[0]\n",
    "    first_in_url = get_poster_url(first_movie['tmdbId'])\n",
    "    if first_in_url == 'NA':\n",
    "        display(HTML('<i>Cannot import tmdbsimple. No movie posters will be displayed!</i>'))\n",
    "    if first_in_url == 'KEY_ERROR':\n",
    "        display(HTML('<i>Key error accessing TMDb API. Check your API key. No movie posters will be displayed!</i>'))\n",
    "    \n",
    "    # display the movies that this user thas rated highly.\n",
    "    display(HTML('<h2>Get recommended movies for user id %s</h2>' % the_id))\n",
    "    display(HTML('<h4>The user has rated the following movies highly:%s</h4>'))\n",
    "    user_html = '<table border=0>'\n",
    "    i = 0\n",
    "    for movie in user_movies:\n",
    "        movie_in_url = get_poster_url(movie['tmdbId'])\n",
    "        movie_title = movie['title']\n",
    "        user_html += '<td><h5>%s</h5><img src=%s width=150></img></td>' % (movie_title, movie_in_url)\n",
    "        i+=1\n",
    "        if i % 5 == 0:\n",
    "            user_html += '</tr><tr>'\n",
    "    user_html += '</tr><tr>'\n",
    "    display(HTML(user_html))\n",
    "    # now display the recommended movies for the user\n",
    "    display(HTML('<br>'))\n",
    "    display(HTML('<h2>Recommended movies:</h2>'))\n",
    "    rec_html = '<table border=0>'\n",
    "    i=0\n",
    "    for rec in recs:\n",
    "        r_in_url = get_poster_url(rec['_source']['tmdbId'])\n",
    "        r_score = rec['_score']\n",
    "        r_title = rec['_source']['title']\n",
    "        rec_html += '<td><h5>%s</h5><img src=%s width=150></img></td><td><td><h5>%2.3f</h5></td>' % (r_title, r_in_url, r_score)\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            rec_html += '</tr><tr>'\n",
    "    rec_html += '</tr></table>'\n",
    "    display(HTML(rec_html))    \n",
    "    \n",
    "def display_similar(the_id, q='*', num=10, index='movie_rec', dt='movies'):\n",
    "    '''\n",
    "    Display query movie, together with similar movies and similarity scores, in a table\n",
    "    '''\n",
    "    movie, recs = get_similar(the_id, q, num, index, dt)\n",
    "    q_in_url = get_poster_url(movie['tmdbId'])\n",
    "    if q_in_url == 'NA':\n",
    "        display(HTML('<i>Cannot import tmdbsimple. No movie posters will be displayed</i>'))\n",
    "    if q_in_url == 'KEY_ERROR':\n",
    "        display(HTML('<i>Key error accessing TMDb API. Check your API key. No movie posters will be displayed!</i>'))\n",
    "    \n",
    "    display(HTML('<h2>Get similar movies for:</h2>'))\n",
    "    sim_html = '<table border=0>'\n",
    "    i=0\n",
    "    for rec in recs:\n",
    "        r_in_url = get_poster_url(rec['_source']['tmdbId'])\n",
    "        r_score = rec['_score']\n",
    "        r_title = rec['_source']['title']\n",
    "        sim_html += '<td><h5>%s<img src=%s width150></img></td></h5>%2.3f</h5></td>' % (r_title, r_in_url, r_score)\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            sim_html += '</tr><tr>'\n",
    "    sim_html += '</tr></table>'\n",
    "    display(HTML(sim_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5(a) Find similar movies for a given movie\n",
    "\n",
    "To start, you can find movies that are similar to a given movie. This similarity score is computed from the model factor vectors for each movie.\n",
    "\n",
    "Using this similarity you can show recommendations along the lines of people who liked this movie also liked these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_similar(2628, num=5)   # 46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll see the power and flexibility that comes from usin a search engine to generate recommendations. Elasticsearch allows you to tweek the movie by recommendation query using any standard search query or filter from free text search through to filters based on time and geo-location.\n",
    "\n",
    "For example, perhaps you want to remove any movies with 'matrix' in the title from the recommendations. You can do this by simply passing a valid Elasticsearch query string to the recommendation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_similar(2628, num=5, q='title:(NOT matrix)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_similar(1, num=5, q='genres:children')  # 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to check the documentation for the Elasticsearch query_string_query and play with the various queries you can construct by passing in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5(b) Find movies to recommend to a user\n",
    "\n",
    "Now, you're ready to generate some movie recommendations, personalized for a specific user.\n",
    "\n",
    "Given a user, you can recommend movies to that user based on the predicted ratings from your model. In a similar manner to the similar movie recommendation using predicted rating score is computed from the model factor vector for the user and the factor vector for each movie. Recall that the collaborative filtering algorithm that, at a high level, we will recommend movies Liked by other users who Liked the same movies as the given user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_user_recs(12, num=5, num_last=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, note that since we are using a very small dataset, the results may not be too good. However, we can see that this user seems to like some sci-fi and some comedy films. The recommended movies fall broadly into these categories and seem to be somewhat reasonable.\n",
    "Next, we can again apply the power of Elasticsearch's filtering capabilities to your recommendation engine. Let's say we only want to recommend movie from the past 5 years. This can be done by adding a date month query to the recommendation function score query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_user_recs(12, num=5, num_last=5, q='release_date:[2012 TO *]')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can see that the recommendation include only recent movies and this time they seem to be heavily titled to sci-fi and fantasy genres.\n",
    "As you did with the similar movies recommendations, feel free to play around with the various queries you could pass into the user recommendation query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Give recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es query\n",
    "# this function is defined in Step 5\n",
    "display_user_recs(12, num=5, num_last=5, q='release_date:[2012 TO *]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
